{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet #wornet loaded\n",
    "import re\n",
    "import json\n",
    "from flask import Flask,render_template, request, redirect, url_for\n",
    "from flask import jsonify, current_app\n",
    "from flask_cors import CORS\n",
    "\n",
    "#Methode definition\n",
    "# https://stackoverflow.com/questions/1883980/find-the-nth-occurrence-of-substring-in-a-string\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "def get_synsets(w):\n",
    "    syns = wordnet.synsets(w)\n",
    "    return syns\n",
    "\n",
    "def get_definitions(syns):\n",
    "    return syns.definition()\n",
    "\n",
    "def get_label(syns):\n",
    "    lemmas = syns.lemmas()\n",
    "    name = lemmas[0].name()\n",
    "    return name\n",
    "\n",
    "def get_class(syns):\n",
    "    return syns.pos()\n",
    "\n",
    "def get_resource(syns):\n",
    "    return syns.offset()  #id for a synset in wordnet db\n",
    "\n",
    "def get_source(syns):\n",
    "    return syns.name()\n",
    "\n",
    "#TODO what if the text is used multiple times?\n",
    "def get_offset(w,text,nth):\n",
    "    offset = find_nth(text, w, nth)\n",
    "    return offset\n",
    "\n",
    "def stop_words_filtering(text):\n",
    "    reg_exp = r\"[a-zA-Z]+\"  #Reguläre Ausdruck, der alle erlaubten Sylabeln enthält. Von \"a\" bis \"z\"\n",
    "                            #von \"A\" bis \"Z\"              \n",
    "    stop_words  = set(stopwords.words('english')) #stopwörter für englisches Text\n",
    "    word_tokens = []\n",
    "    a = re.compile(reg_exp)          \n",
    "    word_tokens = word_tokens + a.findall(text) #löscht die Satzzeichnen im Text\n",
    "    filtered_sentence =  [w for w in word_tokens if not w in stop_words] #löscht die Stopp-Wörter im Text           \n",
    "    return filtered_sentence \n",
    "#filtered_sentence = [] \n",
    "    #for w in word_tokens: \n",
    "      #  if w not in stop_words: \n",
    "    #        filtered_sentence.append(w)\n",
    "\n",
    "#Nested dictionary\n",
    "def build_ressources_candidates(word,syns,offset):\n",
    "    resourcedict = {}\n",
    "    resourcedict[\"description\"] = get_definitions(syns)\n",
    "    resourcedict[\"label\"]       = get_label(syns)\n",
    "    resourcedict[\"offset\"]      = offset\n",
    "    resourcedict[\"resource\"]    = get_resource(syns)\n",
    "    resourcedict[\"source\"]      = get_source(syns)\n",
    "    resourcedict[\"text\"]        = word\n",
    "    resourcedict[\"pos\"]         = get_class(syns) #pos(part of speech oder word class)\n",
    "    return resourcedict   \n",
    "    \n",
    "def build_resources(word,offset):\n",
    "    resources = []\n",
    "    synsets = get_synsets(word)\n",
    "    for syns in synsets:\n",
    "        resources.append(build_ressources_candidates(word,syns,offset))\n",
    "    return resources\n",
    "\n",
    "def build_annot_candidates(word,text,current_word_occurence_count):\n",
    "        offset = get_offset(word,text,current_word_occurence_count)\n",
    "        candidate = {}\n",
    "        candidate[\"offset\"]                = offset\n",
    "        candidate[\"resource_candidates\"]  = build_resources(word,offset)\n",
    "        candidate[\"text\"]                  = word\n",
    "        return candidate\n",
    "def build_annot(text):\n",
    "    annotDict = {}\n",
    "    annot = []\n",
    "    annotDict[\"annotation_candidates\"]   = annot\n",
    "    annotDict[\"text\"]                    = text\n",
    "    Woerter = stop_words_filtering(text)\n",
    "    wordCount = {}\n",
    "    #print('currentWords:',Woerter)\n",
    "    for w in Woerter:\n",
    "        if not (w in wordCount.keys()):\n",
    "            wordCount[w] = 0\n",
    "        wordCount[w] = wordCount[w] + 1\n",
    "        currentWordCount = wordCount[w]\n",
    "        annot.append(build_annot_candidates(w,text, currentWordCount))\n",
    "    return annotDict\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json.dumps(build_annot(\"say that you say\"))\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#d = stop_words_filtering(\"I am a winners, but not a looser 9!\")\n",
    "#print(d)\n",
    "import sys\n",
    "sys.stdout = open('synset.png', 'w')\n",
    "print(get_synsets('Framework'))\n",
    "\n",
    "  # print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:4000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [12/Jan/2020 15:59:27] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jan/2020 15:59:27] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [12/Jan/2020 18:07:12] \"GET /annotApi/pet%20for%20food HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"annotation_candidates\": [{\"offset\": 0, \"resource_candidates\": [{\"description\": \"a domesticated animal kept for companionship or amusement\", \"label\": \"pet\", \"offset\": 0, \"resource\": 1318894, \"source\": \"pet.n.01\", \"text\": \"pet\", \"pos\": \"n\"}, {\"description\": \"a special loved one\", \"label\": \"darling\", \"offset\": 0, \"resource\": 9991867, \"source\": \"darling.n.01\", \"text\": \"pet\", \"pos\": \"n\"}, {\"description\": \"a fit of petulance or sulkiness (especially at what is felt to be a slight)\", \"label\": \"pet\", \"offset\": 0, \"resource\": 7553176, \"source\": \"pet.n.03\", \"text\": \"pet\", \"pos\": \"n\"}, {\"description\": \"using a computerized radiographic technique to examine the metabolic activity in various tissues (especially in the brain)\", \"label\": \"positron_emission_tomography\", \"offset\": 0, \"resource\": 902376, \"source\": \"positron_emission_tomography.n.01\", \"text\": \"pet\", \"pos\": \"n\"}, {\"description\": \"stroke or caress gently\", \"label\": \"pet\", \"offset\": 0, \"resource\": 1425892, \"source\": \"pet.v.01\", \"text\": \"pet\", \"pos\": \"v\"}, {\"description\": \"stroke or caress in an erotic manner, as during lovemaking\", \"label\": \"pet\", \"offset\": 0, \"resource\": 1226600, \"source\": \"pet.v.02\", \"text\": \"pet\", \"pos\": \"v\"}, {\"description\": \"preferred above all others and treated with partiality\", \"label\": \"favored\", \"offset\": 0, \"resource\": 1462882, \"source\": \"favored.s.01\", \"text\": \"pet\", \"pos\": \"s\"}], \"text\": \"pet\"}, {\"offset\": 8, \"resource_candidates\": [{\"description\": \"any substance that can be metabolized by an animal to give energy and build tissue\", \"label\": \"food\", \"offset\": 8, \"resource\": 21265, \"source\": \"food.n.01\", \"text\": \"food\", \"pos\": \"n\"}, {\"description\": \"any solid substance (as opposed to liquid) that is used as a source of nourishment\", \"label\": \"food\", \"offset\": 8, \"resource\": 7555863, \"source\": \"food.n.02\", \"text\": \"food\", \"pos\": \"n\"}, {\"description\": \"anything that provides mental stimulus for thinking\", \"label\": \"food\", \"offset\": 8, \"resource\": 5811214, \"source\": \"food.n.03\", \"text\": \"food\", \"pos\": \"n\"}], \"text\": \"food\"}], \"text\": \"pet for food\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Jan/2020 18:07:15] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "@app.route('/')\n",
    "def index():\n",
    "  return 'Server Works done!'\n",
    "    \n",
    "@app.route('/annotApi/<string:text>')\n",
    "def annotsFunction2(text):\n",
    "    annotation = build_annot(text)\n",
    "    print(json.dumps(annotation))\n",
    "    return json.dumps(annotation)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with app.app_context():\n",
    "   # app.debug = True\n",
    "        from werkzeug.serving import run_simple\n",
    "        run_simple('localhost', 4000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
