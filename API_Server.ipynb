{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize \n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet #wornet loaded\n",
    "import re\n",
    "import json\n",
    "from flask import Flask,render_template, request, redirect, url_for\n",
    "from flask import jsonify, current_app\n",
    "from flask_cors import CORS\n",
    "\n",
    "#Methode definition\n",
    "# https://stackoverflow.com/questions/1883980/find-the-nth-occurrence-of-substring-in-a-string\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    print(\"le debut\",start,n)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    print(start)\n",
    "    return start\n",
    "\n",
    "def get_synsets(w):\n",
    "    syns = wordnet.synsets(w)\n",
    "    return syns\n",
    "\n",
    "def get_definitions(syns):\n",
    "    return syns.definition()\n",
    "\n",
    "def get_label(syns):\n",
    "    lemmas = syns.lemmas()\n",
    "    name = lemmas[0].name()\n",
    "    return name\n",
    "\n",
    "def get_class(syns):\n",
    "    return syns.pos()\n",
    "\n",
    "def get_resource(syns):\n",
    "    return syns.offset()  #id for a synset in wordnet db\n",
    "\n",
    "def get_source(syns):\n",
    "    return syns.name()\n",
    "\n",
    "#TODO what if the text is used multiple times?\n",
    "def get_offset(w,text,nth):\n",
    "    offset = find_nth(text, w, nth)\n",
    "    return offset\n",
    "\n",
    "def stop_words_filtering(text):\n",
    "    reg_exp = r\"[a-zA-Z0-9_-]+\"  #Reguläre Ausdruck, der alle erlaubten Sylabeln enthält. Von \"a\" bis \"z\"\n",
    "                            #von \"A\" bis \"Z\"   \n",
    "    #reg_exp2 = r\"[a-zA-Z0-9]+\"\n",
    "    stop_words  = set(stopwords.words('english')) #stopwörter für englisches Text\n",
    "    word_tokens = []\n",
    "    #word_token2 = []\n",
    "    a = re.compile(reg_exp)\n",
    "    #b = re.compile(reg_exp2)\n",
    "    word_tokens = word_tokens + a.findall(text) #löscht die Satzzeichnen im Text\n",
    "    filtered_sentence = filtered_sentence2 =  [w for w in word_tokens if not w in stop_words] #löscht die Stopp-Wörter im Text\n",
    "    #word_tokens2 = word_tokens2 + b.findall(text) #für die wörter wie pet_food pet-food (-,_ löschen)\n",
    "     \n",
    "    return filtered_sentence \n",
    "#filtered_sentence = [] \n",
    "    #for w in word_tokens: \n",
    "      #  if w not in stop_words: \n",
    "    #        filtered_sentence.append(w)\n",
    "\n",
    "#Nested dictionary\n",
    "def build_ressources_candidates(word,syns,offset):\n",
    "    resourcedict = {}\n",
    "    resourcedict[\"description\"] = get_definitions(syns)\n",
    "    resourcedict[\"label\"]       = get_label(syns)\n",
    "    resourcedict[\"offset\"]      = offset\n",
    "    resourcedict[\"resource\"]    = get_resource(syns)\n",
    "    resourcedict[\"source\"]      = get_source(syns)\n",
    "    resourcedict[\"text\"]        = word\n",
    "    resourcedict[\"pos\"]         = get_class(syns) #pos(part of speech oder word class)\n",
    "    return resourcedict   \n",
    "    \n",
    "def build_resources(word,offset):\n",
    "    resources = []\n",
    "    synsets = get_synsets(word)\n",
    "    if \"-\" in word:\n",
    "        tokens = word.split(\"-\")\n",
    "        for w in tokens:\n",
    "            synsets.extend(get_synsets(w))\n",
    "    elif \"_\" in word:\n",
    "        tokens = word.split(\"_\")\n",
    "        for w in tokens:\n",
    "            synsets.extend(get_synsets(w))    \n",
    "    for syns in synsets:\n",
    "        resources.append(build_ressources_candidates(word,syns,offset))\n",
    "    return resources\n",
    "\n",
    "def build_annot_candidates(word,text,current_word_occurence_count):\n",
    "        offset = get_offset(word,text,current_word_occurence_count)\n",
    "        candidate = {}\n",
    "        candidate[\"offset\"]                = offset\n",
    "        candidate[\"resource_candidates\"]  = build_resources(word,offset)\n",
    "        candidate[\"text\"]                  = word\n",
    "        return candidate\n",
    "def build_annot(text):\n",
    "   \n",
    "    annotDict = {}\n",
    "    annot = []\n",
    "    annotDict[\"annotation_candidates\"]   = annot\n",
    "    annotDict[\"text\"]                    = text\n",
    "    Woerter = stop_words_filtering(text)\n",
    "    wordCount = {}\n",
    "    print('currentWords:',Woerter)\n",
    "    #print(Woerter)\n",
    "    for w in Woerter:\n",
    "        find = 0\n",
    "        if not bool(wordCount): #wenn dictionary leer ist\n",
    "            wordCount[w] = 0\n",
    "            wordCount[w] = wordCount[w] + 1\n",
    "            currentWordCount = wordCount[w]\n",
    "            print(\"Start:\",w)\n",
    "        else:\n",
    "            #if not (w in wordCount.keys()):\n",
    "            #wordCount[w] = 0\n",
    "            for key in wordCount.copy():\n",
    "                #if (key.find(w) == -1):   \n",
    "                if (key.find(w) != -1):\n",
    "                    print(\"ich bin da find ist true:\", key,w)\n",
    "                    wordCount[key] = wordCount[key] + 1\n",
    "                    currentWordCount = wordCount[key]\n",
    "                    find = 1\n",
    "                   # print(\"after\",wordCount[w])\n",
    "                    break\n",
    "            #print(\"ich bin da:\", key,w)\n",
    "            if find != 1:\n",
    "                print(\"not find: \", w)\n",
    "                wordCount[w] = 0\n",
    "                wordCount[w] = wordCount[w] + 1\n",
    "                currentWordCount = wordCount[w]\n",
    "                   # print(\"actual\",currentWordCount)\n",
    "                #print(\"nach\",wordCount[w])\n",
    "        annot.append(build_annot_candidates(w,text, currentWordCount))\n",
    "                    \n",
    "        #if not (w in wordCount.keys()):\n",
    "            #wordCount[w] = 0\n",
    "        \n",
    "            #annot.append(build_annot_candidates(w,text, currentWordCount))\n",
    "    print(wordCount)\n",
    "    return annotDict\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#json.dumps(build_annot(\"say that you say\"))\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#d = stop_words_filtering(\"I am a winners, but not a looser 9!\")\n",
    "#print(d)\n",
    "#import sys\n",
    "#sys.stdout = open('synset.png', 'w')\n",
    "#print(get_synsets('Framework'))\n",
    "\n",
    "  # print()\n",
    "#annotation = build_annot(\"pet-food,petfood and pet\")\n",
    "#dic = {\"pet\":\"food\"}\n",
    "#text = \"pet-food\"\n",
    "#separator = ''\n",
    "#print(bool(dic))\n",
    "#print(text.startswith(\"pet\"))\n",
    "#print(text.find(\"food\"))\n",
    "#print(text.index(\"food\"))\n",
    "#print(enumerate(text))\n",
    "#for index, word in enumerate(nltk.word_tokenize(text)):\n",
    "#    print(index, word)\n",
    "#print(annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:4000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currentWords: ['find', 'lost', 'livestock']\n",
      "Start: find\n",
      "le debut 3 1\n",
      "3\n",
      "not find:  lost\n",
      "le debut 8 1\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jan/2020 16:29:22] \"GET /annotApi/to%20find%20lost%20livestock HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not find:  livestock\n",
      "le debut 13 1\n",
      "13\n",
      "{'find': 1, 'lost': 1, 'livestock': 1}\n",
      "currentWords: ['device', 'could', 'used', 'detective', 'manner', 'say', 'could', 'useful', 'predicting', 'behavioral', 'patterns', 'say', 'criminal', 'offenders', 'students', 'perform', 'research', 'perhaps', 'tracking', 'physical', 'lives', 'athletes', 'top', 'achieving', 'businessman', 'companies', 'isolate', 'expected', 'behaviors', 'employees', 'anticipate', 'employees', 'move', 'work', 'day', 'establish', 'fair', 'true', 'standards']\n",
      "Start: device\n",
      "le debut 4 1\n",
      "4\n",
      "not find:  could\n",
      "le debut 11 1\n",
      "11\n",
      "not find:  used\n",
      "le debut 20 1\n",
      "20\n",
      "not find:  detective\n",
      "le debut 30 1\n",
      "30\n",
      "not find:  manner\n",
      "le debut 40 1\n",
      "40\n",
      "not find:  say\n",
      "le debut 58 1\n",
      "58\n",
      "ich bin da find ist true: could could\n",
      "le debut 11 2\n",
      "70\n",
      "not find:  useful\n",
      "le debut 79 1\n",
      "79\n",
      "not find:  predicting\n",
      "le debut 89 1\n",
      "89\n",
      "not find:  behavioral\n",
      "le debut 100 1\n",
      "100\n",
      "not find:  patterns\n",
      "le debut 111 1\n",
      "111\n",
      "ich bin da find ist true: say say\n",
      "le debut 58 2\n",
      "123\n",
      "not find:  criminal\n",
      "le debut 127 1\n",
      "127\n",
      "not find:  offenders\n",
      "le debut 136 1\n",
      "136\n",
      "not find:  students\n",
      "le debut 147 1\n",
      "147\n",
      "not find:  perform\n",
      "le debut 160 1\n",
      "160\n",
      "not find:  research\n",
      "le debut 168 1\n",
      "168\n",
      "not find:  perhaps\n",
      "le debut 177 1\n",
      "177\n",
      "not find:  tracking\n",
      "le debut 188 1\n",
      "188\n",
      "not find:  physical\n",
      "le debut 201 1\n",
      "201\n",
      "not find:  lives\n",
      "le debut 210 1\n",
      "210\n",
      "not find:  athletes\n",
      "le debut 219 1\n",
      "219\n",
      "not find:  top\n",
      "le debut 231 1\n",
      "231\n",
      "not find:  achieving\n",
      "le debut 235 1\n",
      "235\n",
      "not find:  businessman\n",
      "le debut 245 1\n",
      "245\n",
      "not find:  companies\n",
      "le debut 264 1\n",
      "264\n",
      "not find:  isolate\n",
      "le debut 277 1\n",
      "277\n",
      "not find:  expected\n",
      "le debut 285 1\n",
      "285\n",
      "not find:  behaviors\n",
      "le debut 294 1\n",
      "294\n",
      "not find:  employees\n",
      "le debut 313 1\n",
      "313\n",
      "not find:  anticipate\n",
      "le debut 336 1\n",
      "336\n",
      "ich bin da find ist true: employees employees\n",
      "le debut 313 2\n",
      "347\n",
      "not find:  move\n",
      "le debut 360 1\n",
      "360\n",
      "not find:  work\n",
      "le debut 374 1\n",
      "374\n",
      "not find:  day\n",
      "le debut 379 1\n",
      "379\n",
      "not find:  establish\n",
      "le debut 386 1\n",
      "386\n",
      "not find:  fair\n",
      "le debut 396 1\n",
      "396\n",
      "not find:  true\n",
      "le debut 405 1\n",
      "405\n",
      "not find:  standards\n",
      "le debut 410 1\n",
      "410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/Jan/2020 16:29:57] \"GET /annotApi/the%20device%20could%20be%20used%20in%20a%20detective%20manner%20that%20is%20to%20say%20that%20it%20could%20be%20useful%20to%20predicting%20behavioral%20patterns%20of%20say%20criminal%20offenders,%20students,%20to%20perform%20research%20perhaps%20by%20tracking%20the%20physical%20lives%20of%20athletes%20or%20top%20achieving%20businessman%20or%20for%20companies%20to%20isolate%20expected%20behaviors%20of%20their%20employees%20and%20how%20they%20anticipate%20employees%20to%20move%20during%20a%20work%20day%20to%20establish%20fair%20and%20true%20standards HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device': 1, 'could': 2, 'used': 1, 'detective': 1, 'manner': 1, 'say': 2, 'useful': 1, 'predicting': 1, 'behavioral': 1, 'patterns': 1, 'criminal': 1, 'offenders': 1, 'students': 1, 'perform': 1, 'research': 1, 'perhaps': 1, 'tracking': 1, 'physical': 1, 'lives': 1, 'athletes': 1, 'top': 1, 'achieving': 1, 'businessman': 1, 'companies': 1, 'isolate': 1, 'expected': 1, 'behaviors': 1, 'employees': 2, 'anticipate': 1, 'move': 1, 'work': 1, 'day': 1, 'establish': 1, 'fair': 1, 'true': 1, 'standards': 1}\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "@app.route('/')\n",
    "def index():\n",
    "  return 'Server Works done!'\n",
    "    \n",
    "@app.route('/annotApi/<string:text>')\n",
    "def annotsFunction2(text):\n",
    "    annotation = build_annot(text)\n",
    "    #print(json.dumps(annotation))\n",
    "    return json.dumps(annotation)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    with app.app_context():\n",
    "   # app.debug = True\n",
    "        from werkzeug.serving import run_simple\n",
    "        run_simple('localhost', 4000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
